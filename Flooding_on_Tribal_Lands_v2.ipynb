{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flooding on Tribal Lands\n",
    "## Authors: Bri Stone, Shana Larwrence, Jesse Joseph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Common Issue\n",
    "Spring of 2019 Tribal Nations in South Dakota suffered from extreme flooding that resulted in road loss, land degradation, and property damage. Most reservations do not have funding or resources to replace or rebuild. In the case of road loss, sometimes that road may be the only route causing the possibility of communities or families to be isolated from resources. Property damage for farmers and ranchers can take years to recover, sometimes resulting in total loss. \n",
    "\n",
    "Creating a reservation wide map that contains high to low risk levels will help the tribe identify areas to design preventative measures rather than reacting to the situation. Additionally, landowners can see what parts of their property are at risk, allowing them to use different mitigation strategies.\n",
    "\n",
    "These reasons indicate why it is important for our team to create a universal code that results in a flooding risk map for utilization by all Tribal Entities. We hope the risk map created by the code will help to minimize the issues of land erosion, road and land loss, property damage, land degradation and loss of sacred sites within each reservation. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prior Work\n",
    "There are several organizations that have created where flood maps for most places of the United States, but their intended use is for insurance. One such organization is [FEMA]( https://www.fema.gov/flood-maps), who creates these maps as part of the National Flood Insurance Program (NFIP). Unfortunately when looking at the maps, the information is not very useful to the average person. \n",
    "\n",
    "Another option was with the US Geological Survey (USGS) when they created flood maps that showed the extent of flooding for different flood severities. Although these maps are more intuitive to understand, they are also very hard to find and, in many cases, hard copies are only located in libraries of Universities. The USGS does have some very useful [flood information]( https://www.usgs.gov/faqs/where-can-i-find-flood-maps?qt-news_science_products=0#qt-news_science_products) online, but it is limited in scope.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Method\n",
    "Our team derived it’s method from a similar analysis that was used to create an island wide inundation vulnerability map for the island of St. Lucia, located on the edge of the Caribbean Sea north of Venezuela. The distinctive variables (Land use, Soils, Precipitation, and a Digital Elevation Model) implemented in the document generate risk levels associated with flooding.\n",
    "\n",
    "To determine the infiltration rate, soil type and land-use identify the Runoff Curve Numbers (CN). These numbers range from 30 to 100, with 30 being the lowest and 100 having the highest potential for runoff. Land-use categorizes CNs by soil hydrologic units (A-D) so that intersecting soil and land-use layers identify the correct CN. Using precipitation data, daily amounts for mean annual rainfall uses only data for extreme precipitation events because they result in flooding. \n",
    "\n",
    "Finally, horizontal slopes have the capacity for pooling, thus contributing more to flooding. To compensate, slope has a risk levels of one, two, and six instead of three for horizontal gradients are. After determining risk levels of each variable, the summing of the overlapping values to identify the final risk level results in one through six being low risk and anything greater than nine being high risk.\n",
    "\n",
    "(insert figure 1)\n",
    "\n",
    "Flow Chart from St. Lucia Document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "As is everyone’s goal when setting out to complete a project, not all come to fruition. In the case of our code, it is about 80% complete due to time constraints resulting in the exclusion of precipitation data. One of the big reasons why precipitation data was not included at this time is due to identifying the correct interpolation method and then implementing. Many reservations have very few rain gauges that continuously collect data. For Standing Rock Reservation in particular there are only three gauges on the reservation.\n",
    "\n",
    "Despite precipitation’s absence, the results with just Slope, Soil, and Land-use are rather interesting. When looking at the intermediate layer of CN Risk, calculated from Soil and Land-use, it would appear that the results are the opposite of expectations. With one indicating low risk and three as high risk, the resulting map was inverse of the expected results.\n",
    "\n",
    "(insert Figure 2)\n",
    "\n",
    "The most likely cause stems from CN value assignments made to the Land-use layer. The information that we were working with did not contain a complete array of CN values for all Land-use types. Instead, the data’s focus was agricultural lands. Additionally the St. Lucia document’s study sight was an island so there is also reason to believe that the reassignment of CN value to Risk Level may need alterations for an inland location. Instead of CN values of less than 40 qualifying for low risk, inland might receive more accurate results with CN values less than 50 qualifying for low risk. We believe the same issue exists for slope data, thus we increased the ranges used in the St. Lucia paper by a multiple of 10.\n",
    "\n",
    "Overall, we are proud of the code that we created as an initial base for the analysis. We believe that the foundation created will enable the continuation of building a code that will be able to take user inputs and export a final map with eases. We cannot guarantee that those who will use the code will have a python background, which is why one of our goals was user ease. Down the line, the incorporation of a simple user interface for the code would be possible.\n",
    "\n",
    "(insert Figure 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Moving Forward \n",
    "The project will require the following work:\n",
    "- Inclusion of precipitation data using interpolation\n",
    "- Identifying a way to improve the Land-use portion of CN values\n",
    "- Verifying that risk ranges for each variable are appropriate\n",
    "- User interface?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Orginization\n",
    "### The code is orginized in the following mannor:\n",
    "#### Preperation [go](#Preperation)\n",
    "    1. Packages\n",
    "    2. Vairables\n",
    "    3. Defined Code Blocks\n",
    "#### Analysis [go](#Analysis)\n",
    "    4. Reservation Boundary Identification\n",
    "    5. Land-use Layer Prep\n",
    "    6. Soil Layer Prep\n",
    "    7. CN Risk Layer Creation\n",
    "    8. Slope Layer Prep\n",
    "    9. Flood Risk Layer Creation\n",
    "#### Document Creation [go](#Document-Creation)\n",
    "    10. Final Map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Documents\n",
    "1. Flood Hazard Mapping of St. Lucia (Document)\n",
    "    - By: Vincent Cooper and Jacob Opadeyi (February 2006)\n",
    "        - Chukwuka, Azubuike. (2013). Re: Methodology to \"generate flood risk maps\".. Retrieved from:\n",
    "            -         https://www.researchgate.net/post/Methodology_to_generate_flood_risk_maps/51abca3ed3df3e257b000065/citation/download.\n",
    "2. Hydrologic Soil-Cover Complexes (Document)\n",
    "    - USDA-NRCS\n",
    "    - NEH, Part 650, (EFH), Amend, IA50, NOV, 2007\n",
    "        - https://www.nrcs.usda.gov/Internet/FSE_DOCUMENTS/nrcs142p2_011485.pdf\n",
    "3. Additional Runoff Curve Number Information (Website)\n",
    "    - https://en.wikipedia.org/wiki/Runoff_curve_number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Sources\n",
    "1. Reservation Boundary\n",
    "    - Originator: Branch of Geospatial Support\n",
    "    - Data Name: Land Area Representations (LAR)\n",
    "    - Download format: Zip File with Shapefile(s) (.shp)\n",
    "    - Website: [US Department of the Interior - Indian Affairs](biamaps.doi.gov/bogs/datadownload.html)\n",
    "    - Publication Date: 2018-12-10\n",
    "2. Land-Use Layer\n",
    "    - Originator: U.S. Geological Survey Gap Analysis Program\n",
    "    - Data Name: GAP/LANDFIRE National Terrestrial Ecosystems 2011\n",
    "    - Download format: Zip File with Raster(s) (.tif)\n",
    "    - Website: [USGS - Gap Analysis Project](https://www.usgs.gov/core-science-systems/science-analytics-and-synthesis/gap/science/land-cover-data-download?qt-science_center_objects=0#qt-science_center_objects)\n",
    "    - Publication Date: 2016-05-13\n",
    "3. Soil Layer\n",
    "    - Originator: U.S. Department of Agriculture, Natural Resources Conservation Service\n",
    "    - Data Name: Soil Survey Geographic (SSURGO) database\n",
    "    - Download format: Zip File with Shapefile(s) (.shp)\n",
    "    - Website: [USDA NRCS Web Soil Survey](https://websoilsurvey.sc.egov.usda.gov/)\n",
    "    - Publication Date: 2019-09-16\n",
    "4. Slope Layer\n",
    "    - Originator: USDA/NRCS - National Geospatial Center of Excellence\n",
    "    - Data Name: National Elevation Dataset 30 meter\n",
    "    - Download format: Zip File with Raster(s) (.tif)\n",
    "    - Website: [USDA NRCS Geospatial Data Gateway](https://gdg.sc.egov.usda.gov/)\n",
    "    - Publication Date: 2000-Present"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preperation\n",
    "[return to Code Orginization](#Code-Orginization)\n",
    "\n",
    "The following areas consist of the information required for the entire code to run.\n",
    "1. Packages [go](#1.-Packages)\n",
    "2. Vairables [go](#2.-Vairables)\n",
    "3. Defined Code Blocks [go](#3.-Defined-Code-Blocks)\n",
    "\n",
    "**Note:** Vairables require user input or adjustment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Packages\n",
    "[return to Preperation](#Preperation)\n",
    "\n",
    "These are the different tools to import for the code to run. If you do not already have to package please install. Much of the code can be found at the [GitHub Earth Lab Repository](https://github.com/earthlab/earth-analytics-python-env). The list of packages associated from the repository can be found [here](https://github.com/earthlab/earth-analytics-python-env/blob/master/environment.yml).\n",
    "\n",
    "The only package used and not found in the Earth Lab Repository is [RichDEM](https://richdem.readthedocs.io/en/latest/). This package is used to create a slope layer from the DEM files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imported Packages\n",
    "# Standard Packages\n",
    "import os  # related to file directory\n",
    "from glob import glob  # identifying files within path\n",
    "import sys  # gdal error message\n",
    "\n",
    "from datetime import date\n",
    "import matplotlib as mpl  # plotting data\n",
    "import matplotlib.lines as mlines  # dissolve\n",
    "import matplotlib.pyplot as plt  # plotting data\n",
    "import numpy as np  # used for masking rasters\n",
    "import numpy.ma as ma  # used for masking rasters\n",
    "import pathlib  # related to file directory\n",
    "from pathlib import Path  # related to file directory\n",
    "import pandas as pd  # read CSVs\n",
    "import seaborn as sns  # plotting set-up\n",
    "\n",
    "import earthpy as et  # use\n",
    "import earthpy.plot as ep  # plotting histogram\n",
    "import earthpy.spatial as es  # used for cropping raster\n",
    "import geopandas as gpd  # reading files - vector\n",
    "from osgeo import gdal, ogr  # used to convert raster to shp\n",
    "import rasterio as rio  # reading files - raster\n",
    "from rasterio.features import shapes  # create vector from raster\n",
    "from rasterio.merge import merge  # mosaic rasters\n",
    "from rasterio.plot import plotting_extent  # set extent after cropping\n",
    "from rasterio.plot import show  # plotting mosaic raster\n",
    "# projecting rasters\n",
    "from rasterio.warp import calculate_default_transform, reproject, Resampling\n",
    "import richdem as rd  # open and run DEMs through slope calculations\n",
    "from shapely.geometry import box  # Map Legend\n",
    "from shapely.geometry import Polygon  # clip vector\n",
    "from shapely.geometry import shape  # vector from raster process\n",
    "from shapely.geometry import shape, mapping  # fixing file for overlay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Vairables\n",
    "[return to Preperation](#Preperation)\n",
    "\n",
    "User should confirm that the vairable's folder and file names match those on their system. Vairables are grouped by the following:\n",
    "* Folders [go](#Folder-Vairables)\n",
    "* Specified Original Data [go](#Specific-Originial-Data-Files)\n",
    "* Saved Data Variables [go](#Saved-Data-Variables)\n",
    "* Set Values [go](#Set-Values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Folder Vairables\n",
    "[return to 2. Vairables](#2.-Vairables)\n",
    "\n",
    "These vairables contain the pathways for the code to navigate to folders where either data will be collected or saved. Please only change the red text to fit your folder names. In general each layer type has two folders, an original data folder and a results folder. Only layers that are rasters also have a projection folder. This is to help reduce the risk of confusion for the code.\n",
    "\n",
    "The final layer created of each vairable is then saved in the Risk Layer folder's input folder. This way when looking back on files it is easier to identify which layers were inputs vs. outputs. Any layer that is the result of layers being combined is saved in the Risk Layer folder's output folder.\n",
    "\n",
    "Lastly there are three additional folders: \n",
    "- preped_csv\n",
    "    - The location for CSV tables to be saved for joinig with decitated vairable. Go to [Specific Originial Data](#Specific-Originial-Data-Files) for the particulars of setting up the CSVs.\n",
    "- map_layers\n",
    "    - These layers are cities, road, streams, and rivers that contain area which are used for the creation of the final map.\n",
    "- saved_figs\n",
    "    - This is where all eight figures will be saved. Note that Fig8 is the final figure.\n",
    "\n",
    "For each group the first line is the highest folder to find the remaining data.\n",
    "* EX: org_data_folder is the folder that contains each layer's folder that contains initial data.\n",
    "Please make sure that all data is in one folder and not split in Original Data Folders.\n",
    "* EX: Data downloaded as county1 and count2, all files copied over to county folder\n",
    "\n",
    "**Note:** The Directory is the highest level folder. It should contain everything below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parent Path Exists\n"
     ]
    }
   ],
   "source": [
    "# Directory\n",
    "parent_folder = 'P:\\\\Personal Files\\\\Education\\\\FRCC\\\\NSF_Internship\\\\NSF_Project_Files\\\\Data'\n",
    "if os.path.exists(parent_folder):\n",
    "    os.chdir(os.path.join(parent_folder))\n",
    "    print(\"Parent Path Exists\")\n",
    "else:\n",
    "    os.makedirs(parent_folder)\n",
    "    os.chdir(os.path.join(parent_folder))\n",
    "    print(\"ERROR: Parent path did not exist. Directory has been created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All folder to exist within parent folder\n",
    "\n",
    "# Original Data Folder\n",
    "landuse_original_folder = os.path.join(\"Landuse_Original_Data\")\n",
    "soil_original_folder = os.path.join(\"Soil_Original_Data\")\n",
    "slope_original_folder = os.path.join(\"Slope_Original_Data\")\n",
    "reservation_boundry_original_folder = os.path.join(\n",
    "    \"Reservation_Boundary_Layer\")\n",
    "\n",
    "# Layer Prep Results\n",
    "landuse_result_folder = os.path.join(\"Landuse_Results\")\n",
    "soil_result_folder = os.path.join(\"Soil_Results\")\n",
    "slope_result_folder = os.path.join(\"Slope_Results\")\n",
    "\n",
    "# Risk Layers Results\n",
    "risk_folder = os.path.join(\"Final_Results\")\n",
    "risk_input_folder = os.path.join(risk_folder, \"Input_Layers\")\n",
    "risk_output_folder = os.path.join(risk_folder, \"Output_Layers\")\n",
    "\n",
    "# Reprojections\n",
    "landuse_reproject_folder = os.path.join(\n",
    "    landuse_result_folder, \"Landuse_Projection\")\n",
    "slope_reproject_folder = os.path.join(slope_result_folder, \"Slope_Projection\")\n",
    "\n",
    "# Other\n",
    "preped_csv = os.path.join(\"Preped_CSVs\")\n",
    "map_layers = os.path.join(\"Map_Layers\")\n",
    "saved_figs = os.path.join(\"Figures\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder Exists: Landuse_Original_Data\n",
      "Folder Exists: Soil_Original_Data\n",
      "Folder Exists: Slope_Original_Data\n",
      "Folder Exists: Reservation_Boundary_Layer\n",
      "Folder Exists: Landuse_Results\n",
      "Folder Exists: Soil_Results\n",
      "Folder Exists: Slope_Results\n",
      "Folder Exists: Final_Results\n",
      "Folder Exists: Preped_CSVs\n",
      "Folder Exists: Map_Layers\n",
      "Folder Exists: Figures\n",
      "Folder Exists: Final_Results\\Input_Layers\n",
      "Folder Exists: Final_Results\\Output_Layers\n",
      "Folder Exists: Landuse_Results\\Landuse_Projection\n",
      "Folder Exists: Slope_Results\\Slope_Projection\n"
     ]
    }
   ],
   "source": [
    "# Code to check that all necessary folders within the parent exist. If not the folder is created.\n",
    "first_nested_level_folders = [landuse_original_folder,\n",
    "                              soil_original_folder,\n",
    "                              slope_original_folder,\n",
    "                              reservation_boundry_original_folder,\n",
    "                              landuse_result_folder,\n",
    "                              soil_result_folder,\n",
    "                              slope_result_folder, risk_folder,\n",
    "                              preped_csv,\n",
    "                              map_layers,\n",
    "                              saved_figs]\n",
    "\n",
    "second_nested_level_folders = [risk_input_folder,\n",
    "                               risk_output_folder,\n",
    "                               landuse_reproject_folder,\n",
    "                               slope_reproject_folder]\n",
    "\n",
    "for folder in first_nested_level_folders:\n",
    "    if os.path.exists(folder):\n",
    "        print(\"Folder Exists: %s\" % folder)\n",
    "    else:\n",
    "        os.makedirs(folder)\n",
    "        print(\"Folder Created:  %s\" % folder)\n",
    "\n",
    "for folder in second_nested_level_folders:\n",
    "    if os.path.exists(folder):\n",
    "        print(\"Folder Exists: %s\" % folder)\n",
    "    else:\n",
    "        os.makedirs(folder)\n",
    "        print(\"Folder Created:  %s\" % folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Specific Originial Data Files\n",
    "[return to 2. Vairables](#2.-Vairables)\n",
    "\n",
    "These vairables are for specific files to access. Please only change full file names. If selecting a specific shape file the extension is \".shp\". **Make sure file extensions are Present!**\n",
    "\n",
    "##### Special instructions for CSV files\n",
    "Soil Table: \n",
    "\n",
    "When opening the Muaggatt table in a spreadsheet program it will be a text file that is | seperated (that symbol is on the same keyborad buttion as \\). All of the headers are missing but if you go to the [SSURGO Table Column Descriptions](https://data.nal.usda.gov/system/files/SSURGO_Metadata_-_Table_Column_Descriptions.pdf) and look for Muaggatt all of the headers are there in order. Just add a row at the top of the sheet and copy/paste the Column Physical Name into the first cell of each column. **Do not use the Column Label.** Make sure to save the file as a CSV.\n",
    "\n",
    "Land-use Table:\n",
    "\n",
    "To prepare the Land-use table there are three steps (starting on step 3 the formulas are for use in excel):\n",
    "1. First create a spreadsheet with headders of CN_Join, Description, Sub_Description, Hydrologic_Condition, CN-A, CN-B, CN-C, CN-D. To fill in each column all fields, except the first, are derrived from the 2nd and 3rd item found in [Documnents](#Documents). The first column just runs throught the alphabet and will be used later.\n",
    "2. Each Land-use layer contains two necessary files to create a new one.\n",
    "    - The first is a text file that ends in Attributes. Save a copy of this file for manipulation.\n",
    "    - The second is one that ends with .tif.vat.dbf. The data from this file must be coppied into a new spreadsheet. If more than one Land-Use layer is to be used copy the data from each into one new spreadsheet.\n",
    "3. By now there should be a total of 3 spreadsheets. (CN values, Attributes, coppied dbf) In the attribute file in the very last column give it a headder of \"Present\" and then insert the following code in to all of the cells:\n",
    "    - =IF(IFERROR(VLOOKUP(**A**2,dbf_file.csv!**A1**:**C(last cell number)**,1,FALSE),\"No\")=\"No\",\"No\",\"Yes\")\n",
    "        - Every letter and number that is bold should have a dolar sign in front of it.\n",
    "4. Delete every row that has a \"No\" under Present. This can be acceved easily if the table is sorted by the Present column and scroll down until the \"Yes\" are found or filter out the \"Yes\" and go to the bottom. Then highlight the group of cells that say \"No\" and delete.\n",
    "5. This next part is a bit tedious but create a new column and give each description a CN_Join letter from the first table created. Hopefully in future versions of this code, this part can be automated.\n",
    "6. Add for new columns (CN-A, CN-B, CN-C, CN-D) and in each column us the code below updating the \\*number\\* with the correct number for it's column name.\n",
    "    - =VLOOKUP(**V**2,Hydrologic_SoilCover_Complexes_CN.csv!**A1**:**H(last cell number)**,\\*number\\*,FALSE)\n",
    "    - Every letter and number that is bold should have a dolar sign in front of it.\n",
    "    - Replace the \\*number\\* with one of the following numbers:\n",
    "        - 5 = CN-A\n",
    "        - 6 = CN-B\n",
    "        - 7 = CN-C\n",
    "        - 8 = CN-D\n",
    "7. Copy and paste all values to remove formulas and save as a .csv\n",
    "\n",
    "These two CSV files should be saved in the folder assigned by the preped_csv vairable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specific Originial Data file - file name goes between the \"\"\n",
    "# reservation shapefile name\n",
    "reservation_path = os.path.join(reservation_boundry_original_folder, \"BIA_National_LAR.shp\")\n",
    "\n",
    "all_landuse = glob(os.path.join(landuse_original_folder, \"*.tif\"))\n",
    "\n",
    "all_soil = glob(os.path.join(soil_original_folder, \"*.shp\"))\n",
    "\n",
    "all_dems = glob(os.path.join(slope_original_folder, \"*.tif\"))\n",
    "\n",
    "all_soil_ND = os.path.join(soil_original_folder, \"soilmu_a_nd085.shp\")\n",
    "all_soil_SD = os.path.join(soil_original_folder, \"soilmu_a_sd031.shp\")\n",
    "\n",
    "soil_table = os.path.join(preped_csv, \"muaggatt.csv\")\n",
    "landuse_table = os.path.join(preped_csv, \"GAP_LANDFIRE_Attributes_with_CN.csv\")\n",
    "\n",
    "road_layer = os.path.join(map_layers, \"road100k_l_extract.shp\")\n",
    "city_layer = os.path.join(map_layers, \"gnispop_p_extract.shp\")\n",
    "river_line_layer = os.path.join(map_layers, \"nhd24kst_l_extract.shp\")\n",
    "river_polygon_layer = os.path.join(map_layers, \"nhd24kar_a_extract.shp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saved Data Variables\n",
    "[return to 2. Vairables](#2.-Vairables)\n",
    "\n",
    "These are layers that will be created by the program and then saved in one of the following folder types:\n",
    "* Layer Prep Results\n",
    "* Risk Layers Results\n",
    "* Reprojections\n",
    "\n",
    "Suggested names are present in red, but user may change to suit their needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Land-use\n",
    "landuse_mosaic_outpath = os.path.join(landuse_result_folder, \"landuse_mosaic.tif\")\n",
    "landuse_crop_outpath = os.path.join(landuse_result_folder, \"landuse_mosaic_crop.tif\")\n",
    "landuse_polygon = os.path.join(landuse_result_folder, \"landuse_reclass_poly.shp\")\n",
    "landuse_post_dissolve_clip = os.path.join(landuse_result_folder, \"landuse_poly_dissolve.shp\")\n",
    "final_landuse_layer = os.path.join(risk_input_folder, \"Landuse_CN_Groups.shp\")\n",
    "\n",
    "# Soil\n",
    "concat_soil = os.path.join(soil_result_folder, \"merged_soil.shp\")\n",
    "soil_join_file = os.path.join(soil_result_folder, \"hydro_group_join_soil.shp\")\n",
    "final_soil_layer = os.path.join(risk_input_folder, \"Soil_Hydro_Group.shp\")\n",
    "\n",
    "# Slope\n",
    "dem_mosaic_outpath = os.path.join(slope_result_folder, \"dem_mosaic.tif\")\n",
    "dem_crop_outpath = os.path.join(slope_result_folder, \"dem_mosaic_crop.tif\")\n",
    "slope_outpath = os.path.join(slope_result_folder, \"slope.tif\")\n",
    "slope_reclass_outpath = os.path.join(slope_result_folder, \"slope_reclass.tif\")\n",
    "slope_reclass_poly = os.path.join(slope_result_folder, \"slope_reclass_poly.shp\")\n",
    "slope_post_dissolve = os.path.join(slope_result_folder, \"slope_poly_dissolve.shp\")\n",
    "final_slope_layer = os.path.join(risk_input_folder, \"Slope_Risk.shp\")\n",
    "\n",
    "# Risk Layers\n",
    "CN_soil_land_risk = os.path.join(risk_output_folder, \"CN_Risk.shp\")\n",
    "total_risk = os.path.join(risk_output_folder, \"Total_Risk.shp\")\n",
    "flood_risk = os.path.join(risk_output_folder, \"Flood_Risk.shp\")\n",
    "\n",
    "# Figures\n",
    "landuse_fig_check = os.path.join(saved_figs, \"Fig1_Landuse_vs_Reservation_Boundry_Check.png\")\n",
    "landuse_fig_result = os.path.join(saved_figs, \"Fig2_Reservation_Landuse.png\")\n",
    "soil_fig_check = os.path.join(saved_figs, \"Fig3_Soil_vs_Reservation_Boundry_Check.png\")\n",
    "soil_fig_result = os.path.join(saved_figs, \"Fig4_Soil_Hydro_Group.png\")\n",
    "slope_fig_check = os.path.join(saved_figs, \"Fig6_Slope_Raster_vs_Reservation_Boundry_Check.png\")\n",
    "slope_fig_result = os.path.join(saved_figs, \"Fig7_Reservation_Slope_by_Risk_Value.png\")\n",
    "CN_fig_result = os.path.join(saved_figs, \"Fig5_Soil_Landuse_CN_Risk.png\")\n",
    "final_fig = os.path.join(saved_figs, \"Fig8_Final_Map.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set Values\n",
    "[return to 2. Vairables](#2.-Vairables)\n",
    "\n",
    "These variables are constant values either found within a layer or as a predefined value.\n",
    "\n",
    "**Note:** The projection vairable is set to use ESPG values.Insert the correct number directly after the colon. Use this site to determin ESPG number: https://spatialreference.org/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Used in Analysis\n",
    "# code is set to use espg values use use this site to ID espg number: https://spatialreference.org/\n",
    "site_projection = \"epsg:26914\"\n",
    "reservation_field = \"LARName\"  # field name where reservation name is found\n",
    "reservation_name = \"Standing Rock LAR\"  # name of the reservation\n",
    "\n",
    "# Land-use\n",
    "# Column Headders to keep moving forward\n",
    "landuse_join_head = ['raster_val', 'CN-A', 'CN-B', 'CN-C', 'CN-D', 'geometry']\n",
    "\n",
    "# Soil\n",
    "soil_dissolve_field = 'hydgrpdcd'  # Column Headder to dissolve by\n",
    "# Column Headders to keep during join\n",
    "soil_join_head = ['AREASYMBOL', 'SPATIALVER', 'musym',\n",
    "                  'muname', 'MUKEY', soil_dissolve_field, 'geometry']\n",
    "# Slope\n",
    "# everything with a percent slope greater than this number will be reclassified as 1\n",
    "slope_reclass_val_1 = 5\n",
    "# everything with a percent slope less than this number will be reclassified as 6\n",
    "slope_reclass_val_2 = 1\n",
    "\n",
    "# Result layers\n",
    "CN_upper_limit = 70  # everything with a CN greater than this number will be assigned a 3\n",
    "CN_lower_limit = 40  # everything with a CN less than this number will be assigned a 1\n",
    "risk_upper_limit = 6  # everything with a combined risk greater than this number will be assigned a 3\n",
    "risk_lower_limit = 4  # everything with a combined risk less than this number will be assigned a 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used in Map Creation\n",
    "dpi_value = 400\n",
    "reservation_map_name = \"Standing Rock\"  # name of reservation for the map display\n",
    "road_field = \"RTTYP\"  # field of road type\n",
    "road_vairable = [\"C\", \"M\", \"S\", \"U\"]  # road type primary road code\n",
    "river_field = \"FCODE\"  # field name of stream type\n",
    "river_vairable = [46000, 46006]  # stream code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Defined Code Blocks\n",
    "[return to Preperation](#Preperation)\n",
    "\n",
    "This is code that will be called upon later in the analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reprojection Defined Code for Rasters\n",
    "def reproject_et(inpath, outpath, new_crs):\n",
    "    dst_crs = new_crs  # new projection\n",
    "\n",
    "    with rio.open(inpath) as src:\n",
    "        transform, width, height = calculate_default_transform(\n",
    "            src.crs, dst_crs, src.width, src.height, *src.bounds)\n",
    "        kwargs = src.meta.copy()\n",
    "        kwargs.update({\n",
    "            'crs': dst_crs,\n",
    "            'transform': transform,\n",
    "            'width': width,\n",
    "            'height': height\n",
    "        })\n",
    "\n",
    "        with rio.open(outpath, 'w', **kwargs) as dst:\n",
    "            for i in range(1, src.count + 1):\n",
    "                reproject(\n",
    "                    source=rio.band(src, i),\n",
    "                    destination=rio.band(dst, i),\n",
    "                    src_transform=src.transform,\n",
    "                    src_crs=src.crs,\n",
    "                    dst_transform=transform,\n",
    "                    dst_crs=dst_crs,\n",
    "                    resampling=Resampling.nearest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def around(coords, precision=5):\n",
    "    result = []\n",
    "    try:\n",
    "        return round(coords, precision)\n",
    "    except TypeError:\n",
    "        for coord in coords:\n",
    "            result.append(around(coord, precision))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_precision(geometry, precision=5):\n",
    "    geojson = mapping(geometry)\n",
    "    geojson['coordinates'] = around(geojson['coordinates'], precision)\n",
    "    return shape(geojson)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def raster_mosaic_and_mask(raster_folder, projection, yes):\n",
    "    \"\"\"mosaicing 2 or more .tif files from a folder and then applying a mask\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        raster_folder: path\n",
    "            folder that contains all of the .tif files to be mosaiced\n",
    "\n",
    "        projection: epsg value\n",
    "            the projection that everything should be in.\n",
    "        \n",
    "        yes: string\n",
    "            indicate yes or no to apply a mask\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        masked_mosaic: Pandas DataFrame\n",
    "            updated Pandas DataFrame with the year set as the index\n",
    "        \n",
    "        first_tif_meta: meta data\n",
    "            returns the meta data from the first tif file\n",
    "    \"\"\"  \n",
    "    tif_file_glob = glob(os.path.join(raster_folder, \"*.tif\"))\n",
    "    folder_name = os.path.basename(raster_folder)\n",
    "    base=folder_name[:-8]\n",
    "    reproject_folder=base+\"_Projection\"\n",
    "        \n",
    "    # check reproject folder exists\n",
    "    if os.path.exists(os.path.join(raster_folder,reproject_folder)):\n",
    "        print(\"Reprojection folder exists\")\n",
    "    else:\n",
    "        os.makedirs(os.path.join(raster_folder,reproject_folder))\n",
    "        print(\"ERROR: Reprojection folder did not exist. Reprojection folder has been created.\")\n",
    "\n",
    "    # reproject\n",
    "    for ned in tif_file_glob:\n",
    "        reproject_et(inpath = os.path.join(ned), \n",
    "                     outpath = os.path.join(raster_folder,reproject_folder,os.path.basename(ned)), \n",
    "                     new_crs = projection)\n",
    "\n",
    "    projected_path = glob(os.path.join(raster_folder,reproject_folder,\"*.tif\"))\n",
    "\n",
    "    tifs_to_mosaic = []\n",
    "\n",
    "    # Open Rasters\n",
    "    for ned in projected_path:\n",
    "        src = rio.open(ned)\n",
    "        tifs_to_mosaic.append(src)\n",
    "\n",
    "    # merge\n",
    "    tif_mosaic, tif_out_trans = merge(tifs_to_mosaic)\n",
    "\n",
    "    # getting meta\n",
    "    with rio.open(projected_path[0]) as src:\n",
    "        first_tif_data = src.read()\n",
    "        first_tif_meta = src.profile\n",
    "\n",
    "    # getting shape size (x,y)\n",
    "    tif_width_meta = tif_mosaic.shape[2]\n",
    "    tif_height_meta = tif_mosaic.shape[1]\n",
    "\n",
    "    # more meta information\n",
    "    first_tif_meta['width'] = tif_width_meta\n",
    "    first_tif_meta['height'] = tif_height_meta\n",
    "    first_tif_meta['transform'] = tif_out_trans\n",
    "\n",
    "    # closing nolonger needed files\n",
    "    for raster in tifs_to_mosaic:\n",
    "        raster.close()\n",
    "\n",
    "    if yes=='yes':\n",
    "        # applying mask to data\n",
    "        masking_mosaic = np.where(tif_mosaic < 0, True, False)\n",
    "        masked_mosaic = np.ma.masked_array(tif_mosaic, masking_mosaic)\n",
    "    elif yes=='no':\n",
    "        masked_mosaic=tif_mosaic\n",
    "    else:\n",
    "        print('Plese list yes or no as the 3rd parameter for the function to run.')\n",
    "    \n",
    "    return masked_mosaic, first_tif_meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_to_vector(file_outpath, savefile, projection):\n",
    "    \"\"\"Opening and converting a csv file to a Pandas Dataframe with\n",
    "       the year in the file name as an index.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        file_outpath: path\n",
    "            list of the two folders where the tif flies are located.\n",
    "            \n",
    "        savefile: path\n",
    "            list of the two folders where the tif flies are located.\n",
    "\n",
    "        projection: epsg value\n",
    "            the projection that everything should be in.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        nothing, instead it saves the file in the designated path\n",
    "            \n",
    "            \n",
    "            landuse_polygon = os.path.join(landuse_result_folder, \"landuse_reclass_poly.shp\")\n",
    "    \"\"\"  \n",
    "    mask = None\n",
    "    with rio.Env():\n",
    "        with rio.open(file_outpath) as src:\n",
    "            image = src.read(1)  # first band\n",
    "            # print(src.crs)\n",
    "            results = (\n",
    "                {'properties': {'raster_val': v}, 'geometry': s}\n",
    "                for i, (s, v)\n",
    "                in enumerate(\n",
    "                    shapes(image, mask=mask, transform=src.transform)))\n",
    "    \n",
    "    geometry = list(results)\n",
    "    \n",
    "    poly_from_raster = gpd.GeoDataFrame.from_features(\n",
    "        geometry, crs=projection)\n",
    "    poly_from_raster.loc[:, \"geometry\"] = poly_from_raster[\"geometry\"].apply(\n",
    "        lambda x: layer_precision(x, precision=5))\n",
    "    poly_from_raster.to_file(savefile)\n",
    "    \n",
    "    return print(\"Saved to %s\" % savefile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "[return to Code Orginization](#Code-Orginization)\n",
    "\n",
    "4. Reservation Boundary Identification [go](#4.-Reservation-Boundary-Identification)\n",
    "5. Land-use Layer Prep [go](#5.-Land-use-Layer-Prep)\n",
    "6. Soil Layer Prep [go](#6.-Soil-Layer-Prep)\n",
    "7. CN Risk Layer Creation [go](#7.-CN-Risk-Layer-Creation)\n",
    "8. Slope Layer Prep [go](#8.-Slope-Layer-Prep)\n",
    "9. Flood Risk Layer Creation [go](#9.-Flood-Risk-Layer-Creation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Reservation Boundary Identification\n",
    "[return to Analysis](#Analysis)\n",
    "\n",
    "This set of code isolates a particular reservation from the BIA shapefile and updates the projection to match the indicated ESPG value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Reservation\n",
    "reservation_boundry = gpd.read_file(reservation_path)\n",
    "reservation_aoi = reservation_boundry[reservation_boundry[reservation_field] == reservation_name]\n",
    "reservation_projected = reservation_aoi.to_crs(site_projection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "del reservation_boundry\n",
    "del reservation_aoi\n",
    "del reservation_path\n",
    "del reservation_field\n",
    "del reservation_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Land-use Layer Prep\n",
    "[return to Analysis](#Analysis)\n",
    "\n",
    "Using the St. Lucia document as a guide the land-use raster data goes through the following steps:\n",
    "1. Reprojecting and Mosaicing all Layers together\n",
    "2. Reducing extent to match the Reservation boundry\n",
    "3. Converting to a shapefile and tightening the file to the Reservation boundary\n",
    "4. Joining the CN values from the approperate CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "landuse_mosaic_masked, landuse_meta = raster_mosaic_and_mask(landuse_result_folder,site_projection,\"yes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "landuse_mosaic_squeezed = landuse_mosaic_masked.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with rio.open(landuse_mosaic_outpath, 'w', **landuse_meta) as dst:\n",
    "    dst.write(landuse_mosaic_squeezed, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with rio.open(landuse_mosaic_outpath) as landuse_src:\n",
    "    # Crop raster data to boundary\n",
    "    landuse_data_crop, landuse_crop_meta = es.crop_image(\n",
    "        landuse_src, reservation_projected)\n",
    "# Define plotting extent using cropped array and transform from metadata\n",
    "landuse_crop_plot_extent = plotting_extent(\n",
    "    landuse_data_crop[0], landuse_crop_meta[\"transform\"])\n",
    "\n",
    "landuse_data_crop[landuse_data_crop<0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "landuse_crop_fig, ax = plt.subplots()\n",
    "\n",
    "ep.plot_bands(landuse_data_crop,\n",
    "              ax=ax,\n",
    "              title=\"Landuse Cropped to the Extent\\nof the Reservation Shapefile\\nValues = Landuse Code\",\n",
    "              scale=False,\n",
    "              cmap=\"tab20c_r\",\n",
    "              extent=landuse_crop_plot_extent)  # Use plotting extent from cropped array\n",
    "\n",
    "reservation_projected.plot(color='None',\n",
    "                   edgecolor='black',\n",
    "                   linewidth=3,\n",
    "                   ax=ax)\n",
    "plt.show()\n",
    "plt.draw()\n",
    "landuse_crop_fig.savefig(landuse_fig_check, dpi=dpi_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "landuse_data_crop_squeezed = landuse_data_crop.squeeze()\n",
    "with rio.open(landuse_crop_outpath, 'w', **landuse_crop_meta) as dst:\n",
    "    dst.write(landuse_data_crop_squeezed, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clearing out old data round 2\n",
    "del landuse_mosaic_outpath\n",
    "del landuse_data_crop\n",
    "del landuse_data_crop_squeezed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_to_vector(landuse_crop_outpath,landuse_polygon,site_projection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "landuse_polygon_open = gpd.read_file(landuse_polygon)\n",
    "landuse_dissolve_value = landuse_polygon_open[['raster_val', 'geometry']]\n",
    "landuse_dissolve = landuse_dissolve_value.dissolve(by='raster_val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del landuse_polygon\n",
    "del landuse_polygon_open\n",
    "del landuse_dissolve_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "landuse_buffer = landuse_dissolve.buffer(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "landuse_poly_clip = gpd.clip(landuse_buffer, reservation_projected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "landuse_poly_clip = landuse_poly_clip.reset_index(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "landuse_poly_clip.to_file(landuse_post_dissolve_clip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "landuse_table_open = pd.read_csv(landuse_table)\n",
    "landuse_post_dissolve_clip_open = gpd.read_file(landuse_post_dissolve_clip)\n",
    "landuse_post_dissolve_clip_open['raster_val'] = landuse_post_dissolve_clip_open.raster_val.astype(\n",
    "    int)\n",
    "landuse_table_open['Value'] = landuse_table_open.Value.astype(int)\n",
    "landuse_table_open.rename(columns={'Value': 'raster_val'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "landuse_join = landuse_post_dissolve_clip_open.merge(\n",
    "    landuse_table_open, on='raster_val', how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "landuse_join_reduced = landuse_join[landuse_join_head]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "landuse_result_fig, ax1 = plt.subplots()\n",
    "landuse_join_reduced.plot(cmap='tab20c_r',\n",
    "                     column='raster_val',\n",
    "                     categorical=True,\n",
    "                     edgecolor='None',\n",
    "                     legend=False,\n",
    "                     ax=ax1)\n",
    "ax1.set(title=\"Landuse as a Vector File\\nEach color is a different landuse type\")\n",
    "plt.show()\n",
    "plt.draw()\n",
    "landuse_result_fig.savefig(landuse_fig_result, dpi=dpi_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save File\n",
    "landuse_join_reduced.to_file(final_landuse_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del landuse_buffer\n",
    "del landuse_join\n",
    "del landuse_join_head\n",
    "del landuse_join_reduced\n",
    "del landuse_poly_clip\n",
    "del landuse_table\n",
    "del landuse_table_open"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Soil Layer Prep\n",
    "[return to Analysis](#Analysis)\n",
    "\n",
    "Using the St. Lucia document as a guide the Soil shapefiles go through the following steps:\n",
    "1. Reprojecting and Merging all layers together\n",
    "2. Clipping the layer to match the Reservation boundry\n",
    "3. Joining the layer with Hydrologic Units from the approperate CSV\n",
    "4. Dissolving boundaries by Hydrologic Units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soil_list = []\n",
    "for vect in all_soil:\n",
    "    soil_vect = gpd.read_file(vect)\n",
    "    soil_proj = soil_vect.to_crs(site_projection)\n",
    "    soil_list.append(soil_proj)\n",
    "\n",
    "# soil_SD=gpd.read_file(all_soil_SD)\n",
    "# soil_ND=gpd.read_file(all_soil_ND)\n",
    "#soil_SD_proj = soil_SD.to_crs(site_projection)\n",
    "#soil_ND_proj = soil_ND.to_crs(site_projection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_soil = gpd.GeoDataFrame(pd.concat(soil_list))\n",
    "# merged_soil=gpd.GeoDataFrame(pd.concat([soil_SD_proj,soil_ND_proj]))\n",
    "merged_soil.to_file(concat_soil)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del merged_soil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_soil_open = gpd.read_file(concat_soil)\n",
    "soil_reduced = gpd.clip(concat_soil_open, reservation_projected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soil_table_open = pd.read_csv(soil_table)\n",
    "soil_reduced['MUKEY']=soil_reduced.MUKEY.astype(str)\n",
    "soil_table_open['mukey']=soil_table_open.mukey.astype(str)\n",
    "soil_table_open.rename(columns={'mukey':'MUKEY'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soil_crop_fig, ax1 = plt.subplots()\n",
    "soil_reduced.plot(cmap='tab20c_r',\n",
    "                  column='MUKEY',\n",
    "                  categorical=True,\n",
    "                  edgecolor='None',\n",
    "                  legend=False,\n",
    "                  ax=ax1)\n",
    "ax1.set(title=\"Soil Cropped to the Extent\\nof the Reservation Shapefile\\nEach color is a different soil type\")\n",
    "plt.draw()\n",
    "soil_crop_fig.savefig(soil_fig_check, dpi=dpi_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soil_join = soil_reduced.merge(soil_table_open, on='MUKEY', how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soil_join_reduced = soil_join[soil_join_head]\n",
    "soil_join_reduced.to_file(soil_join_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del concat_soil\n",
    "del soil_join_reduced\n",
    "del soil_join_head\n",
    "del soil_table\n",
    "del soil_table_open\n",
    "del concat_soil_open"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soil_reduced = gpd.read_file(soil_join_file)\n",
    "soil_dissolve_field_cut = soil_dissolve_field[:10]\n",
    "soil_dissolve_value = soil_reduced[[soil_dissolve_field_cut, 'geometry']]\n",
    "soil_dissolve = soil_dissolve_value.dissolve(by=soil_dissolve_field_cut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soil_clip = gpd.clip(soil_dissolve, reservation_projected)\n",
    "soil_clip = soil_clip.reset_index(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soil_result_fig, ax1 = plt.subplots()\n",
    "soil_clip.plot(cmap='Paired_r',\n",
    "               column=soil_dissolve_field_cut,\n",
    "               categorical=True,\n",
    "               edgecolor='None',\n",
    "               legend=True,\n",
    "               ax=ax1)\n",
    "ax1.set(title=\"Soil by Hydrologic Group\")\n",
    "plt.draw()\n",
    "soil_result_fig.savefig(soil_fig_result, dpi=dpi_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soil_clip.to_file(final_soil_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del soil_join_file\n",
    "del soil_reduced\n",
    "del soil_clip\n",
    "del soil_dissolve\n",
    "del soil_dissolve_value\n",
    "del soil_dissolve_field\n",
    "del soil_dissolve_field_cut"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. CN Risk Layer Creation\n",
    "[return to Analysis](#Analysis)\n",
    "\n",
    "Using the St. Lucia document as a guide, this set of code combins the Land-use and Soil layers to create the CN_risk layer. Both layers are intersected and based on Hydrologic Code the approperate CN value from the Land-use iss assigned to the polygon. CN Risk Value is then assigned based on CN range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_soil_layer_open = gpd.read_file(final_soil_layer)\n",
    "final_landuse_layer_open = gpd.read_file(final_landuse_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_soil_layer_open.loc[:, \"geometry\"] = final_soil_layer_open[\"geometry\"].apply(\n",
    "    lambda x: layer_precision(x, precision=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_landuse_layer_open.loc[:, \"geometry\"] = final_landuse_layer_open[\"geometry\"].apply(\n",
    "    lambda x: layer_precision(x, precision=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CN_risk_Join = gpd.overlay(final_soil_layer_open, final_landuse_layer_open, how='intersection')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "soil_landuse_join = os.path.join(\"ArcGIS\", \"soil_land_join.shp\")\n",
    "CN_risk_Join = gpd.read_file(soil_landuse_join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "CN_risk = CN_risk_Join.assign(CN=0, CN_risk=0)\n",
    "CN_risk = CN_risk.fillna(-9999)\n",
    "\n",
    "column_old=['CN-A','CN-B','CN-C','CN-D']\n",
    "column_new=['CN_A','CN_B','CN_C','CN_D']\n",
    "settoint=[CN_risk.CN_A.astype(int),CN_risk.CN_B.astype(int),\n",
    "          CN_risk.CN_C.astype(int),CN_risk.CN_D.astype(int)]\n",
    "\n",
    "for acolumn_old, acolumn_new, aint in zip(column_old,column_new,settoint):\n",
    "    CN_risk.rename(columns={acolumn_old:acolumn_new}, inplace=True)\n",
    "    CN_risk[acolumn_new] = aint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "hydrolic_code = ['A', 'A/B', 'A/C', 'A/D', 'B', 'B/C', 'B/D', 'C', 'C/D', 'D']\n",
    "function = [CN_risk['CN_A'], ((CN_risk['CN_A']+CN_risk['CN_B'])/2), ((CN_risk['CN_A']+CN_risk['CN_C'])/2),\n",
    "            ((CN_risk['CN_A']+CN_risk['CN_D']) /\n",
    "             2), CN_risk['CN_B'], ((CN_risk['CN_B']+CN_risk['CN_C'])/2),\n",
    "            ((CN_risk['CN_B']+CN_risk['CN_D']) /\n",
    "             2), CN_risk['CN_C'], ((CN_risk['CN_C']+CN_risk['CN_D'])/2),\n",
    "            CN_risk['CN_D']]\n",
    "\n",
    "for hcode, func in zip(hydrolic_code, function):\n",
    "    CN_risk['CN'] = np.where(\n",
    "        (CN_risk['Hydrologic'] == hcode), func, CN_risk['CN'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FID_Landus</th>\n",
       "      <th>raster_val</th>\n",
       "      <th>CN_A</th>\n",
       "      <th>CN_B</th>\n",
       "      <th>CN_C</th>\n",
       "      <th>CN_D</th>\n",
       "      <th>FID_Soil_H</th>\n",
       "      <th>Hydrologic</th>\n",
       "      <th>geometry</th>\n",
       "      <th>CN</th>\n",
       "      <th>CN_risk</th>\n",
       "      <th>CN_Risk</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>-32768</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>B</td>\n",
       "      <td>MULTIPOLYGON (((281046.209 5091757.064, 281016...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>-32768</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>C</td>\n",
       "      <td>MULTIPOLYGON (((280089.490 5091786.962, 280059...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>-32768</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>D</td>\n",
       "      <td>MULTIPOLYGON (((305382.728 5090890.038, 305352...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>91</td>\n",
       "      <td>0</td>\n",
       "      <td>71</td>\n",
       "      <td>81</td>\n",
       "      <td>89</td>\n",
       "      <td>0</td>\n",
       "      <td>A</td>\n",
       "      <td>MULTIPOLYGON (((362576.537 5126563.063, 362576...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>91</td>\n",
       "      <td>0</td>\n",
       "      <td>71</td>\n",
       "      <td>81</td>\n",
       "      <td>89</td>\n",
       "      <td>2</td>\n",
       "      <td>B</td>\n",
       "      <td>MULTIPOLYGON (((360663.101 5131148.130, 360663...</td>\n",
       "      <td>71.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>25</td>\n",
       "      <td>584</td>\n",
       "      <td>77</td>\n",
       "      <td>85</td>\n",
       "      <td>90</td>\n",
       "      <td>92</td>\n",
       "      <td>1</td>\n",
       "      <td>A/D</td>\n",
       "      <td>MULTIPOLYGON (((273003.796 5105539.786, 272973...</td>\n",
       "      <td>84.5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>25</td>\n",
       "      <td>584</td>\n",
       "      <td>77</td>\n",
       "      <td>85</td>\n",
       "      <td>90</td>\n",
       "      <td>92</td>\n",
       "      <td>2</td>\n",
       "      <td>B</td>\n",
       "      <td>MULTIPOLYGON (((374595.310 5139277.690, 374595...</td>\n",
       "      <td>85.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>25</td>\n",
       "      <td>584</td>\n",
       "      <td>77</td>\n",
       "      <td>85</td>\n",
       "      <td>90</td>\n",
       "      <td>92</td>\n",
       "      <td>3</td>\n",
       "      <td>C</td>\n",
       "      <td>MULTIPOLYGON (((374625.207 5139712.564, 374595...</td>\n",
       "      <td>90.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>25</td>\n",
       "      <td>584</td>\n",
       "      <td>77</td>\n",
       "      <td>85</td>\n",
       "      <td>90</td>\n",
       "      <td>92</td>\n",
       "      <td>4</td>\n",
       "      <td>C/D</td>\n",
       "      <td>MULTIPOLYGON (((284010.883 5101591.410, 284006...</td>\n",
       "      <td>91.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>25</td>\n",
       "      <td>584</td>\n",
       "      <td>77</td>\n",
       "      <td>85</td>\n",
       "      <td>90</td>\n",
       "      <td>92</td>\n",
       "      <td>5</td>\n",
       "      <td>D</td>\n",
       "      <td>MULTIPOLYGON (((374386.028 5141018.141, 374386...</td>\n",
       "      <td>92.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>142 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     FID_Landus  raster_val  CN_A  CN_B  CN_C  CN_D  FID_Soil_H Hydrologic  \\\n",
       "0             0      -32768     0     0     0     0           2          B   \n",
       "1             0      -32768     0     0     0     0           3          C   \n",
       "2             0      -32768     0     0     0     0           5          D   \n",
       "3             1          91     0    71    81    89           0          A   \n",
       "4             1          91     0    71    81    89           2          B   \n",
       "..          ...         ...   ...   ...   ...   ...         ...        ...   \n",
       "137          25         584    77    85    90    92           1        A/D   \n",
       "138          25         584    77    85    90    92           2          B   \n",
       "139          25         584    77    85    90    92           3          C   \n",
       "140          25         584    77    85    90    92           4        C/D   \n",
       "141          25         584    77    85    90    92           5          D   \n",
       "\n",
       "                                              geometry    CN  CN_risk  CN_Risk  \n",
       "0    MULTIPOLYGON (((281046.209 5091757.064, 281016...   0.0        0        1  \n",
       "1    MULTIPOLYGON (((280089.490 5091786.962, 280059...   0.0        0        1  \n",
       "2    MULTIPOLYGON (((305382.728 5090890.038, 305352...   0.0        0        1  \n",
       "3    MULTIPOLYGON (((362576.537 5126563.063, 362576...   0.0        0        1  \n",
       "4    MULTIPOLYGON (((360663.101 5131148.130, 360663...  71.0        0        3  \n",
       "..                                                 ...   ...      ...      ...  \n",
       "137  MULTIPOLYGON (((273003.796 5105539.786, 272973...  84.5        0        3  \n",
       "138  MULTIPOLYGON (((374595.310 5139277.690, 374595...  85.0        0        3  \n",
       "139  MULTIPOLYGON (((374625.207 5139712.564, 374595...  90.0        0        3  \n",
       "140  MULTIPOLYGON (((284010.883 5101591.410, 284006...  91.0        0        3  \n",
       "141  MULTIPOLYGON (((374386.028 5141018.141, 374386...  92.0        0        3  \n",
       "\n",
       "[142 rows x 12 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CN_risk[\"CN_Risk\"] = 2\n",
    "CN_risk.loc[CN_risk.CN > CN_upper_limit, 'CN_Risk'] = 3\n",
    "CN_risk.loc[CN_risk.CN < CN_lower_limit, 'CN_Risk'] = 1\n",
    "CN_risk.loc[CN_risk.CN == -9999, 'CN_Risk'] = -9999\n",
    "CN_risk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CN_dissolve_prep = CN_risk[['CN_Risk', 'geometry']]\n",
    "CN_dissolve = CN_dissolve_prep.dissolve(by='CN_Risk')\n",
    "CN_dissolve = CN_dissolve.reset_index(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CN_value_fig, ax1 = plt.subplots()\n",
    "CN_dissolve.plot(cmap='autumn_r',\n",
    "                 column='CN_Risk',\n",
    "                 categorical=True,\n",
    "                 edgecolor='None',\n",
    "                 legend=True,\n",
    "                 ax=ax1)\n",
    "ax1.set(title=\"Risk Level of Soil and Landuse\")\n",
    "plt.draw()\n",
    "CN_value_fig.savefig(CN_fig_result, dpi=dpi_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CN_dissolve.to_file(CN_soil_land_risk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del final_landuse_layer\n",
    "del final_soil_layer\n",
    "del final_soil_layer_open\n",
    "del final_landuse_layer_open\n",
    "del CN_risk_Join\n",
    "del CN_risk\n",
    "del CN_dissolve_prep\n",
    "del CN_dissolve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Slope Layer Prep\n",
    "[return to Analysis](#Analysis)\n",
    "\n",
    "Using the St. Lucia document as a guide the DEMs go through the following steps:\n",
    "1. Reprojecting and Mosaicing all Layers together\n",
    "2. Reducing extent to match the Reservation boundry\n",
    "3. Running a Slope Analysis to determine percent slope\n",
    "4. Converting to a shapefile and tightening the file to the Reservation boundary\n",
    "6. Assigning Risk Value based on slope range\n",
    "7. Dissolving boundaries by Risk Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dem_mosaic_masked, dem_meta = raster_mosaic_and_mask(slope_result_folder,site_projection,\"yes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dem_mosaic_squeezed = dem_mosaic_masked.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write raster object to folder\n",
    "with rio.open(dem_mosaic_outpath, 'w', **dem_meta) as dst:\n",
    "    dst.write(dem_mosaic_squeezed, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with rio.open(dem_mosaic_outpath) as dem_src:\n",
    "    # Crop raster data to boundary\n",
    "    dem_data_crop, dem_crop_meta = es.crop_image(\n",
    "        dem_src, reservation_projected)\n",
    "# Define plotting extent using cropped array and transform from metadata\n",
    "dem_crop_plot_extent = plotting_extent(\n",
    "    dem_data_crop[0], dem_crop_meta[\"transform\"])\n",
    "\n",
    "dem_data_crop[dem_data_crop<0]=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slope_crop_fig, ax = plt.subplots()\n",
    "\n",
    "ep.plot_bands(dem_data_crop,\n",
    "              ax=ax,\n",
    "              title=\"DEMs Cropped to the Extent\\nof the Reservation Shapefile\\nValues = Elevation in meters\",\n",
    "              scale=False,\n",
    "              cmap=\"gray\",\n",
    "              extent=dem_crop_plot_extent)  # Use plotting extent from cropped array\n",
    "\n",
    "reservation_projected.plot(color='None',\n",
    "                   edgecolor='teal',\n",
    "                   linewidth=3,\n",
    "                   ax=ax)\n",
    "plt.draw()\n",
    "slope_crop_fig.savefig(slope_fig_check, dpi=dpi_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dem_data_crop_squeezed = dem_data_crop.squeeze()\n",
    "with rio.open(dem_crop_outpath, 'w', **dem_crop_meta) as dst:\n",
    "    dst.write(dem_data_crop_squeezed, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clearing out old data round 2\n",
    "del dem_data_crop\n",
    "del dem_data_crop_squeezed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dem_crop_open = rd.LoadGDAL(dem_crop_outpath)\n",
    "slope_calculated = rd.TerrainAttribute(\n",
    "    dem_crop_open, attrib='slope_percentage')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slope_reclass = slope_calculated\n",
    "slope_reclass[slope_reclass > slope_reclass_val_1] = 1000\n",
    "slope_reclass[(slope_reclass >= slope_reclass_val_2)\n",
    "              & (slope_reclass < 999)] = 2\n",
    "slope_reclass[slope_reclass < slope_reclass_val_2] = 6\n",
    "slope_reclass[slope_reclass == 1000] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slope_orignal_calc = rd.TerrainAttribute(\n",
    "    dem_crop_open, attrib='slope_percentage')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with rio.open(slope_outpath, 'w', **dem_crop_meta) as dst:\n",
    "    dst.write(slope_orignal_calc, 1)\n",
    "with rio.open(slope_reclass_outpath, 'w', **dem_crop_meta) as dst:\n",
    "    dst.write(slope_reclass, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clearing out old data round 3\n",
    "del slope_calculated\n",
    "del slope_reclass\n",
    "del slope_orignal_calc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_to_vector(slope_reclass_outpath,slope_reclass_poly,site_projection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slope_reclass_poly_open = gpd.read_file(slope_reclass_poly)\n",
    "slope_dissolve_value = slope_reclass_poly_open[['raster_val', 'geometry']]\n",
    "slope_dissolve = slope_dissolve_value.dissolve(by='raster_val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slope_dissolve.to_file(slope_post_dissolve)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slope_buffer = slope_dissolve.buffer(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slope_clip = gpd.clip(slope_buffer, reservation_projected)\n",
    "slope_clip = slope_clip.reset_index(0)\n",
    "slope_clip.rename(columns={'raster_val': 'Slope_Risk'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "slope_result_fig, ax1 = plt.subplots()\n",
    "slope_clip.plot(cmap='autumn_r',\n",
    "                     column='Slope_Risk',\n",
    "                     categorical=True,\n",
    "                     edgecolor='None',\n",
    "                     legend=True,\n",
    "                     ax=ax1)\n",
    "ax1.set(title=\"Slope Reclassified to Risk Levels\")\n",
    "plt.draw()\n",
    "slope_result_fig.savefig(slope_fig_result, dpi=dpi_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save File\n",
    "slope_clip.to_file(final_slope_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del slope_reclass_poly_open\n",
    "del slope_dissolve_value\n",
    "del slope_dissolve\n",
    "del slope_clip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Flood Risk Layer Creation\n",
    "[return to Analysis](#Analysis)\n",
    "\n",
    "Using the St. Lucia document as a guide, this set of code combines CN Risk and Slope. Both layers are intersected and then the sum of their risk values is totaled. Flood Risk is then assigned based on Total Risk range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_slope_layer_open = gpd.read_file(final_slope_layer)\n",
    "CN_soil_land_risk_open = gpd.read_file(CN_soil_land_risk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_slope_layer_open.rename(columns={'raster_val': 'Slope_Risk'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_slope_layer_open.loc[:, \"geometry\"] = final_slope_layer_open[\"geometry\"].apply(\n",
    "    lambda x: layer_precision(x, precision=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_risk_Join = gpd.overlay(CN_soil_land_risk_open, final_slope_layer_open,\n",
    "                              how=\"intersection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_risk_Join.to_file(total_risk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del final_slope_layer\n",
    "del CN_soil_land_risk\n",
    "del final_slope_layer_open\n",
    "del CN_soil_land_risk_open\n",
    "del total_risk_Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_risk_open = gpd.read_file(total_risk)\n",
    "calc_risk = total_risk_open.assign(total_risk=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calc_risk[\"Total_Risk\"] = calc_risk[\"CN_Risk\"] + calc_risk[\"Slope_Risk\"]\n",
    "calc_risk.loc[calc_risk.CN_risk == -9999, 'Total_risk'] = -9999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calc_risk_dissolve_prep = calc_risk[['Total_Risk', 'geometry']]\n",
    "calc_risk_dissolve = calc_risk_dissolve_prep.dissolve(by='Total_Risk')\n",
    "calc_risk_dissolve = calc_risk_dissolve.reset_index(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_risk = calc_risk_dissolve.assign(Risk_Level=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_risk[\"Risk_Level\"] = 2\n",
    "final_risk.loc[final_risk.Total_risk > risk_upper_limit, 'Risk_Level'] = 3\n",
    "final_risk.loc[final_risk.Total_risk < risk_lower_limit, 'Risk_Level'] = 1\n",
    "final_risk.loc[final_risk.Total_risk == -9999, 'Risk_Level'] = -9999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_risk_dissolve_prep = final_risk[['Risk_Level', 'geometry']]\n",
    "final_risk_dissolve = final_risk_dissolve_prep.dissolve(by='Risk_Level')\n",
    "final_risk_dissolve = final_risk_dissolve.reset_index(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_risk_dissolve = final_risk_dissolve.assign(\n",
    "    Flood_Risk=\"empty\", Area=0, Acres=0, Sq_Mi=0)\n",
    "final_risk_dissolve.loc[final_risk_dissolve.Risk_Level ==\n",
    "                        3, 'Flood_Risk'] = \"High\"\n",
    "final_risk_dissolve.loc[final_risk_dissolve.Risk_Level ==\n",
    "                        2, 'Flood_Risk'] = \"Medium\"\n",
    "final_risk_dissolve.loc[final_risk_dissolve.Risk_Level ==\n",
    "                        1, 'Flood_Risk'] = \"Low\"\n",
    "final_risk_dissolve.loc[final_risk_dissolve.Risk_Level == -\n",
    "                        9999, 'Flood_Risk'] = \"No Value\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_risk_dissolve[\"Area\"] = final_risk_dissolve['geometry'].area\n",
    "final_risk_dissolve[\"Acres\"] = final_risk_dissolve['Area']/4046.85642\n",
    "final_risk_dissolve[\"Sq_Mi\"] = final_risk_dissolve['Area']/2590000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_risk_dissolve.to_file(flood_risk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del total_risk_open\n",
    "del calc_risk\n",
    "del calc_risk_dissolve_prep\n",
    "del calc_risk_dissolve\n",
    "del final_risk\n",
    "del final_risk_dissolve_prep\n",
    "del final_risk_dissolve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Creation\n",
    "[return to Code Orginization](#Code-Orginization)\n",
    "### 10. Final Map\n",
    "\n",
    "The set of code below results in a map being created showing the results of the analysis above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust font size and style of all plots in notebook with seaborn\n",
    "sns.set(font_scale=1.5, style=\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vector Layers\n",
    "# cities\n",
    "cities = gpd.read_file(city_layer)\n",
    "cities_prj = cities.to_crs(site_projection)\n",
    "bound_cities = gpd.clip(cities_prj, reservation_projected)\n",
    "# roads\n",
    "road_file = gpd.read_file(road_layer)\n",
    "road_prj = road_file.to_crs(site_projection)\n",
    "bound_road = gpd.clip(road_prj, reservation_projected)\n",
    "road_primary = bound_road  # [bound_road[road_field] == road_vairable]\n",
    "# rivers\n",
    "river_file = gpd.read_file(river_polygon_layer)\n",
    "river_prj = river_file.to_crs(site_projection)\n",
    "bound_rivers = gpd.clip(river_prj, reservation_projected)\n",
    "\n",
    "stream_file = gpd.read_file(river_line_layer)\n",
    "stream_prj = stream_file.to_crs(site_projection)\n",
    "bound_stream = gpd.clip(stream_prj, reservation_projected)\n",
    "streams = bound_stream[bound_stream[river_field].isin(river_vairable)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flood_high = ['High']\n",
    "flood_medium = ['Medium']\n",
    "flood_low = ['Low']\n",
    "flood_results = gpd.read_file(flood_risk)\n",
    "flood_results_high = flood_results[flood_results['Flood_Risk'].isin(flood_high)]\n",
    "flood_results_medium = flood_results[flood_results['Flood_Risk'].isin(flood_medium)]\n",
    "flood_results_low = flood_results[flood_results['Flood_Risk'].isin(flood_low)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Raster Layers\n",
    "with rio.open(dem_mosaic_outpath) as dem_src:\n",
    "    # Crop raster data to boundary\n",
    "    dem_data_crop, dem_crop_meta = es.crop_image(\n",
    "        dem_src, reservation_projected)\n",
    "\n",
    "# Define plotting extent using cropped array and transform from metadata\n",
    "dem_crop_plot_extent = plotting_extent(\n",
    "    dem_data_crop[0], dem_crop_meta[\"transform\"])\n",
    "\n",
    "\n",
    "squeezed_dem = dem_data_crop.squeeze()\n",
    "\n",
    "dem_hillshade = es.hillshade(squeezed_dem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the data\n",
    "import matplotlib.patches as patches\n",
    "final_flood_risk_map, ax = plt.subplots(figsize=(15, 15))\n",
    "\n",
    "# Hillshade\n",
    "ep.plot_bands(dem_hillshade,\n",
    "              cmap='gray',\n",
    "              extent=dem_crop_plot_extent,\n",
    "              ax=ax,\n",
    "              cbar=False)\n",
    "\n",
    "# Flood Risk\n",
    "flood_results_high.plot(color='red',\n",
    "                     column='Flood_Risk',\n",
    "                     edgecolor='None',\n",
    "                     label='High Risk',\n",
    "                     alpha=0.6,\n",
    "                     legend=True,\n",
    "                     ax=ax)\n",
    "\n",
    "flood_results_medium.plot(color='orange',\n",
    "                     column='Flood_Risk',\n",
    "                     edgecolor='None',\n",
    "                     label='Medium Risk',\n",
    "                     alpha=0.7,\n",
    "                     legend=True,\n",
    "                     ax=ax)\n",
    "\n",
    "flood_results_low.plot(color='yellow',\n",
    "                     column='Flood_Risk',\n",
    "                     edgecolor='None',\n",
    "                     label='Low Risk',\n",
    "                     alpha=0.7,\n",
    "                     legend=True,\n",
    "                     ax=ax)\n",
    "\n",
    "# Cities\n",
    "bound_cities.plot(color='black',\n",
    "                  marker='.',\n",
    "                  markersize=200,\n",
    "                  label='Cities',\n",
    "                  legend=True,\n",
    "                  ax=ax,\n",
    "                  zorder=3)\n",
    "\n",
    "# Roads\n",
    "bound_road.plot(color='black',\n",
    "                label='Roads',\n",
    "                ax=ax,\n",
    "                legend=True,\n",
    "                zorder=2)\n",
    "\n",
    "# Rivers\n",
    "streams.plot(color='blue',\n",
    "                   linewidth=2,\n",
    "                   label='Rivers',\n",
    "                   ax=ax,\n",
    "                   legend=True,\n",
    "                   zorder=1)\n",
    "\n",
    "bound_rivers.plot(color='cyan',\n",
    "                  edgecolor='blue',\n",
    "                  ax=ax,\n",
    "                  zorder=1)\n",
    "\n",
    "rect1 = patches.Rectangle((0, 0), 1, 1, linewidth=1.5,\n",
    "                          label=\"High Risk\", color=\"red\")\n",
    "rect2 = patches.Rectangle((0, 0), 1, 1, linewidth=1.5,\n",
    "                          label=\"Medium Risk\", color=\"orange\")\n",
    "rect3 = patches.Rectangle((0, 0), 1, 1, linewidth=1.5,\n",
    "                          label=\"Low Risk\", color=\"yellow\")\n",
    "plt.gca().add_patch(rect1)\n",
    "plt.gca().add_patch(rect2)\n",
    "plt.gca().add_patch(rect3)\n",
    "\n",
    "# Legend\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "ax.legend(handles, labels)\n",
    "# plt.legend('fontsize'=40)\n",
    "# Title\n",
    "tyear = str(date.today().year)\n",
    "ax.set_title(reservation_map_name+\"\\nFlood Risk Map, \"+tyear, fontsize=40)\n",
    "\n",
    "plt.show()\n",
    "plt.draw()\n",
    "final_flood_risk_map.savefig(final_fig, dpi=dpi_value)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
